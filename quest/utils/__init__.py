import torch
import math
from typing import Optional

import quest._kernels as _kernels
from quest.utils.utils import TensorLayout
from quest.utils.kv_cache import KvCache
from quest.utils.controller import InferenceController
from quest.utils.decode_wrapper import BatchDecodeWithPagedKVCacheWrapper

__all__ = [
    'TensorLayout',
    'KvCache',
    'InferenceController',
    "BatchDecodeWithPagedKVCacheWrapper",
    "append_kv",
    "prefill_forward",
    "decode_estimate",
    "decode_topk",
    "decode_sparse_attn",
    "rms_norm_forward",
    "apply_rope_in_place",
    "get_offloaded_token_info",
]

def apply_rope_in_place(
    q: torch.Tensor,
    k: torch.Tensor,
    past_kv_len: int,
    rope_scale: Optional[float] = None,
    rope_theta: Optional[float] = None,
):
    """
    Semantics of `apply_rope_in_place`:
    Apply RoPE (Relative Positional Encoding) in-place.
    On q, k which is generated by GEMM. Layout is naturally NHD.

    Args:
        q: Shape: `[N, H, D]`.
        k: Shape: `[N, H, D]`. 
        past_kv_len: Length of past KV cache. Used to calculate frequency.
    """
    if rope_scale is None:
        rope_scale = 1.0
    if rope_theta is None:
        rope_theta = 1e4
    _kernels.apply_rope_in_place(
        q,
        k,
        past_kv_len,
        rope_scale,
        rope_theta,
    )

def rms_norm_forward(
    input: torch.Tensor,
    weight: torch.Tensor,
    epsilon: float,
) -> torch.Tensor:
    o = torch.empty_like(input, dtype=input.dtype, device=input.device)
    f = _kernels.rms_norm_forward
    f(
        input,
        weight,
        o,
        epsilon,
    )
    return o

def append_kv(
    k: torch.Tensor,
    v: torch.Tensor,
    iController: InferenceController,
    layer_idx: int,
):
    """
    Semantics of `append_kv`:
    Append new generated k/v into kv cache and meta data cache.
    Automatically dispatch to Prefill / Decode Kernel

    Notations for shapes:
    `B`: batch size
    `N`: number of heads
    `D`: head dimension
    `L`: number of layers
    `MAXLEN`: maximum length of the KV cache

    Args:
        k: Shape: `[B, N, D]`. Key projection (`X @ W_k`).
        v: Shape: `[B, N, D]`. Value projection (`X @ W_v`).
        iController: InferenceController object, which contains all needed information.
        layer_idx: Layer index of the KV cache.
    """
    seq_len = k.size(0)
    if seq_len > 1:
        _kernels.append_kv_cache_prefill(
            k,
            v,
            iController.kv_cache.buf_layer(layer_idx),
            iController.kv_indices_with_last,
            iController.kv_indptr_for_append,
            iController.kv_cache.last_page_len,
            iController.kv_last_page_idx,
            iController.metadata_cache.buf_layer(layer_idx),
            iController.metadata_indices,
            iController.metadata_indptr_for_append,
            iController.metadata_cache.last_page_len,
            iController.metadata_last_page_idx,
            iController.layout
        )
    else:
        _kernels.append_kv_cache_decode(
            k,
            v,
            iController.kv_cache.buf_layer(layer_idx),
            iController.kv_indices_with_last,
            iController.kv_indptr_for_append,
            iController.kv_cache.last_page_len,
            iController.kv_last_page_idx,
            iController.metadata_cache.buf_layer(layer_idx),
            iController.metadata_indices,
            iController.metadata_indptr_for_append,
            iController.metadata_cache.last_page_len,
            iController.metadata_last_page_idx,
            iController.layout
        )

def prefill_forward(
    q: torch.Tensor,
    iController: InferenceController,
    layer_idx: int,
    rope_scale: Optional[float] = None,
    rope_theta: Optional[float] = None,
) -> torch.Tensor:
    """
    Semantics of `prefill_forward`:
    New genrated K/Vs are already in the kv cache and meta data cache (well-maintained).
    Perform FlashInfer Self-Attention with Casual Attention.
    Note that we not have position shift and current version not support Prefill Optimization.

    Notations for shapes:
    `B`: batch size
    `N`: number of heads
    `D`: head dimension
    `L`: number of layers
    `MAXLEN`: maximum length of the KV cache

    Args:
        q: Shape: `[B, N, D]`. Key projection (`X @ W_k`).
        iController: InferenceController object, which contains all needed information.
        layer_idx: Layer index of the KV cache.
    """
    if rope_scale is None:
        rope_scale = 1.0
    if rope_theta is None:
        rope_theta = 1e4

    f = _kernels.prefill_with_paged_kv_cache
    o = f(
        q,
        iController.kv_cache.buf_layer(layer_idx),
        iController.kv_indices_with_last,
        iController.kv_cache.last_page_len,
        True, # Casual
        iController.layout,
        False, # FP16 Accumulator for 4090
        rope_scale,
        rope_theta,
    )
    return o

def decode_estimate(
    q: torch.Tensor,
    iController: InferenceController,
    layer_idx: int,
) -> torch.Tensor:
    """
    Semantics of `decode_estimate`:
    When decoding, estimate the attention score for each page.
    Enhanced to consider ALL offloaded tokens (prompt + previously decoded).

    Notations for shapes:
    `B`: batch size
    `N`: number of heads
    `D`: head dimension
    `L`: number of layers
    `MAXLEN`: maximum length of the KV cache

    Args:
        q: Shape: `[B, N, D]`. Key projection (`X @ W_k`).
        iController: InferenceController object, which contains all needed information.
        layer_idx: Layer index of the KV cache.
    """
    f = _kernels.estimate_attn_score
    
    # Enhanced: Consider all offloaded pages (prompt + decoded tokens)
    # Get the number of available pages for estimation (excluding current page)
    total_metadata_pages = iController.metadata_cache.seqlen
    available_pages_for_estimation = max(0, total_metadata_pages - 1)
    
    if available_pages_for_estimation == 0:
        # No pages available for estimation (only current page exists)
        return torch.empty((iController.num_heads, 0), dtype=q.dtype, device=q.device)
    
    # Create output tensor for all available offloaded pages
    o = torch.empty((iController.num_heads, available_pages_for_estimation), dtype=q.dtype, device=q.device)
    
    f(
        q,
        o,
        iController.metadata_cache.buf_layer(layer_idx),
        iController.metadata_indices,
        iController.metadata_indptr_for_append,
        iController.metadata_cache.last_page_len, # One entry delta is considered by kernel-level implementation
        iController.metadata_last_page_idx,
        iController.layout,
    )
    return o

def decode_topk(
    estimated_attn_score: torch.Tensor,
    iController: InferenceController,
):
    """
    Semantics of `decode_topk`:
    select top-k pages with highest attention score.
    Enhanced to work with all offloaded tokens (prompt + decoded).

    Notations for shapes:
    `B`: batch size
    `N`: number of heads
    `D`: head dimension
    `L`: number of layers
    `MAXLEN`: maximum length of the KV cache

    Args:
        estimated_attn_score: Shape: `[N, available_pages]`. Attention scores for available pages.
        iController: InferenceController object, which contains all needed information.
    """
    # Enhanced: Handle dynamic number of available pages
    if estimated_attn_score.size(1) == 0:
        # No pages to select from
        return
    
    # Calculate actual page budget based on available pages
    available_pages = estimated_attn_score.size(1)
    actual_page_budget = min(iController.inference_page_budget - 1, available_pages)
    
    if actual_page_budget <= 0:
        return
    
    f = _kernels.topk_filtering
    f(
        estimated_attn_score,
        iController.kv_indices_without_last,
        iController.topk_dout_buffer,
        iController.topk_dindices_buffer,
        iController.topk_buf,
        actual_page_budget,
    )

def decode_sparse_attn(
    q: torch.Tensor,
    iController: InferenceController,
    layer_idx: int,
    topk_indices: torch.Tensor,
    rope_scale: Optional[float] = None,
    rope_theta: Optional[float] = None,
) -> torch.Tensor:
    """
    Semantics of `decode_sparse_attn`:
    Excute self-attention only on the selected pages (Top-k output)

    Notations for shapes:
    `B`: batch size
    `N`: number of heads
    `D`: head dimension
    `L`: number of layers
    `MAXLEN`: maximum length of the KV cache

    Args:
        q: Shape: `[B, N, D]`. Key projection (`X @ W_k`).
        iController: InferenceController object, which contains all needed information.
        layer_idx: Layer index of the KV cache.
        topk_indices: Shape: `[N, page_budget-1]`. Top-k indices.
    """
    o = torch.empty_like(q, dtype=q.dtype, device=q.device)
    iController._decode_handler.forward(
        q,
        o,
        iController.kv_cache.buf_layer(layer_idx),
        topk_indices,
        iController.kv_indptr_for_approx_decode,
        iController.kv_cache.last_page_len,
        iController.kv_last_page_idx,
        rope_scale,
        rope_theta,
    )
    return o

def get_offloaded_token_info(iController: InferenceController) -> dict:
    """
    Enhanced utility function to get detailed information about offloaded tokens.
    
    This function provides comprehensive information about the current state of 
    offloaded tokens, including prompt tokens and previously decoded tokens,
    which is useful for debugging and monitoring the Quest KV cache compression.
    
    Args:
        iController: InferenceController object containing cache state
        
    Returns:
        dict: Comprehensive information about offloaded token state
    """
    composition = iController.get_token_composition()
    
    # Calculate page utilization
    total_pages = len(iController.kv_cache.indicies)
    metadata_pages = len(iController.metadata_cache.indicies)
    page_size = iController.page_size
    
    # Estimate tokens per page (accounting for partial last page)
    if total_pages > 0:
        last_page_utilization = iController.kv_cache.last_page_len / page_size
        full_pages = max(0, total_pages - 1)
        estimated_tokens_in_cache = full_pages * page_size + iController.kv_cache.last_page_len
    else:
        last_page_utilization = 0.0
        estimated_tokens_in_cache = 0
    
    # Calculate availability for attention estimation
    available_for_estimation = max(0, metadata_pages - 1)
    
    return {
        # Token composition
        'token_composition': composition,
        
        # Page information
        'cache_pages': {
            'total_kv_pages': total_pages,
            'total_metadata_pages': metadata_pages,
            'page_size': page_size,
            'last_page_utilization': last_page_utilization,
            'estimated_tokens_in_cache': estimated_tokens_in_cache,
        },
        
        # Attention estimation availability
        'attention_estimation': {
            'pages_available_for_estimation': available_for_estimation,
            'can_perform_estimation': available_for_estimation > 0,
            'current_page_budget': getattr(iController, 'inference_page_budget', None),
        },
        
        # Offloading status
        'offloading_status': {
            'prompt_fully_offloaded': composition['is_in_prefill'] == False,
            'decoded_tokens_count': composition['decoded_tokens_count'],
            'all_tokens_available_for_attention': available_for_estimation > 0,
        }
    }